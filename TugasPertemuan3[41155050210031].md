## TUGAS 3
Nama: Sifa Maryam Rahman

NPM:	41155050210031

Kelas:	TIF A1

1.Berikut adalah hasil praktik dari video youtube https://youtu.be/5wwXKtLkyqs?si=fn88eveu_qbCC6b3 

1.1 Pengenalan komponen Decision Tree: root, node, leaf
Decision Tree sendiri merupakan algoritma Machine Learning yang termasuk dalam algoritma Supervised Learning dan Decision Tree sendiri merupakan salah satu dari berbagai macam algortima Supervised Learning yang dapat menyelesaikan Classification dan Regression.
![image](https://github.com/user-attachments/assets/dc58a6a8-f61a-41a9-a3ab-523d3a66e49b)

Alur dari gambar diatas adalah sebagai berikut.
	1. Apakah Anda takut dengan teknologi?
 
     Jika ya
     
  2. maka alurnya akan menuju ke pertanyaan kedua: "Apakah ayahmu kaya?"
     
	3. Jika "ya", maka pilihannya adalah menggunakan Apple (macOS).
 
	4. Jika "tidak", maka pilihannya adalah Google Chrome OS.
 
	5. Jika "tidak", maka dilanjutkan dengan pertanyaan: "Peduli dengan privasi?"
 
	6. Jika "ya", maka dilanjutkan ke pertanyaan berikutnya: "Punya kehidupan?"
 
	7. Jika "ya", maka pilihannya adalah Ubuntu (Linux).
 
	8. Jika "tidak", maka pilihannya adalah Arch Linux.
 
	9. Jika "tidak" peduli dengan privasi", maka pilihannya adalah Windows.

Struktur pohon terbalik dalam konteks ilmu komputer atau diagram keputusan terdiri dari tiga elemen utama: root node, internal node, dan leaf node. Berikut adalah penjelasan lebih detail dari masing-masing elemen:

1. Root Node (Simpul Akar)
Definisi: Root node adalah simpul yang berada di bagian paling atas dari pohon terbalik. Ini adalah titik awal atau elemen pertama dari struktur pohon.

2. Internal Node (Simpul Internal)
Definisi: Internal node adalah simpul yang berada di antara root node dan leaf node. Simpul ini memiliki cabang atau anak-anak dan bukan simpul akhir.

3. Leaf Node (Simpul Daun)
Definisi: Leaf node adalah simpul yang berada di bagian paling bawah dari pohon terbalik, dan tidak memiliki cabang atau anak-anak. Ini adalah simpul akhir yang tidak lagi bercabang.

Alurnya seperti ini:
Root Node: Simpul pertama → Internal Node: Simpul menengah yang berfungsi sebagai penghubung → Leaf Node: Simpul akhir yang menentukan hasil atau keputusan.

1.2 Pengenalan Gini Impurity

![image](https://github.com/user-attachments/assets/407a51c0-bf98-462f-b7dd-a75725f3a43a)

Gambar tersebut menunjukkan konsep Gini Impurity, yang sering digunakan dalam decision tree (pohon keputusan) pada algoritma seperti CART (Classification and Regression Trees). Pada gambar, kita bisa melihat pemisahan data menggunakan garis vertikal di grafik scatter plot, dimana dua kelompok berbeda direpresentasikan oleh titik biru dan titik hijau.
Untuk menghitung Gini Impurity pada ruas kiri, mari kita ikuti langkah-langkah berikut.
![image](https://github.com/user-attachments/assets/0a580312-3407-439a-9927-a1df2a19886a)

Di mana:
	
 Pi adalah proporsi dari setiap kelas dalam ruas kiri.
	
 n adalah jumlah kelas yang ada dalam ruas kiri.

Dengan P(biru) = 1, maka:

G=1−(12)=1−1=0
Pada ruas kanan, terdapat dua kelas: biru dan hijau. Proporsi dari setiap kelas dijelaskan sebagai berikut:

P(biru) = 1/6

P(hijau) = 5/6

Maka, kita menghitung Gini Impurity untuk ruas kanan dengan memasukkan nilai proporsi tersebut:

G = 1 - ((1/6)^2+ (5/6)^2 )  

G = 1 - (1/36+ 25/36)  

G = 1 - 26/36 = 1 – 0.722 = 0.278

Jadi, Gini Impurity untuk ruas kanan adalah 0.278, menunjukkan bahwa ada sedikit ketidaksempurnaan dalam pemisahan ini (karena ada titik biru yang sedikit tercampur dengan mayoritas hijau).
Setelah memisahkan data, kita menghitung Gini Impurity rata-rata dari kedua ruas (kiri dan kanan). Gini Impurity rata-rata dihitung dengan menggunakan bobot berdasarkan jumlah titik di masing-masing ruas.


Perhitungan dilakukan dengan rumus berikut:

G =  4/(4+6) x 0 + 6/(4+6) x 0.278

G =  4/10 x 0 + 6/10 x 0.278

G = 0 + 0.1688 = 0.1668
Jadi, Average Gini Impurity setelah pemisahan adalah 0.1668.

1.3 Pengenalan Information Gain

![image](https://github.com/user-attachments/assets/1c57a767-b3ef-440b-9044-5585d3f0bcbf)

Information gain adalah konsep yang digunakan untuk mengukur seberapa banyak ketidakpastian (impurity) yang berkurang setelah melakukan pemisahan (splitting) pada sebuah dataset, khususnya dalam algoritma decision tree.

1.4 Membangun Decision Tree

![image](https://github.com/user-attachments/assets/13d6ba53-ba45-47a2-8291-f8012468e197)

Gambar diatas menjelaskan sebagai berikut.
	
 1. Terdapat 5 data buah dengan atribut Color, diameter, dan label (Apple, Grape, Lemon).
 2. Setiap baris mewakili satu contoh data: misalnya, buah berwarna hijau dengan diameter 3 memiliki label Apple.
 3. Ada beberapa pertanyaan potensial yang bisa digunakan untuk memisahkan data, seperti:
 4. Apakah warna hijau?
 5. Apakah warna kuning?
 6. Apakah warna merah?
 7. Apakah diameternya lebih kecil atau sama dengan 1?
 8. Apakah diameternya lebih kecil atau sama dengan 3?
 9. Pemisahan ini bertujuan untuk mengelompokkan data sedemikian rupa sehingga masing-masing grup lebih homogen.

Pada gambar diatas, terlihat bahwa pohon keputusan memilih pemisahan (splitting) berdasarkan kriteria dengan "highest information gain" atau keuntungan informasi tertinggi. Splitting ini ditentukan berdasarkan pengurangan impurity terbesar pada data. 

1.5 Persiapan dataset: Iris Dataset
 ![image](https://github.com/user-attachments/assets/dfdf3186-f502-44b2-b78d-41e33854718a)

1.6 Training model Decision Tree Classifier
 ![image](https://github.com/user-attachments/assets/2dd36020-b6be-41de-94c1-2658da0e3b15)

1.7 Visualisasi model Decision Tree
 ![image](https://github.com/user-attachments/assets/c91cb5b3-52ad-4721-b1f4-5546539987da)

1.8 Evaluasi model Decision Tree
 ![image](https://github.com/user-attachments/assets/398eb91b-ba19-4968-aced-5be9b2d78267)

 
2. Berikut adalah hasil praktik dari video youtube https://youtu.be/yKovaQ6tyV8?si=HnHG6kcoCsDwvo_0  
2.1 Proses training model Machine Learning secara umum
 ![image](https://github.com/user-attachments/assets/4f679d6d-76d2-4872-ad39-ecf11a93c55a)

Training set digunakan untuk melakukan pelatihan (training) model machine learning. Setelah proses pelatihan selesai, model yang sudah dilatih ini disebut sebagai trained model. Trained model tersebut kemudian digunakan untuk melakukan prediksi terhadap data baru yang belum pernah dilihat oleh model sebelumnya. Dengan kata lain, trained model memanfaatkan pengetahuan yang didapat selama proses pelatihan untuk memprediksi hasil pada input baru yang tidak ada dalam training set.

2.2Pengenalan Ensemble Learning
 ![image](https://github.com/user-attachments/assets/39ee985d-2c45-4781-beab-809f98702eea)

Ensemble Learning adalah metode dalam machine learning di mana beberapa model (yang sering disebut base models atau base learners) digabungkan untuk menghasilkan prediksi yang lebih baik daripada prediksi dari satu model tunggal. Dengan menggabungkan prediksi dari beberapa model, ensemble learning biasanya mampu meningkatkan akurasi dan generalizability dari sistem prediksi.
Pada gambar di atas, jika kita memadukan sejumlah model machine learning yang berbeda jenisnya, maka teknik ini sering disebut sebagai heterogeneous ensemble learning atau ensemble learning yang heterogen.
Di sisi lain, jika kita memadukan sejumlah model machine learning yang sejenis atau menggunakan algoritma yang sama, maka ini dikenal sebagai homogeneous ensemble learning atau ensemble learning yang homogen.
Perbedaan Utama:
	
  1. Heterogeneous Ensemble Learning:
    	Menggabungkan model yang berbeda (misalnya, pohon keputusan, SVM, KNN).
      
      Bertujuan untuk memanfaatkan keunggulan masing-masing model yang beragam.
    	
      Cocok digunakan ketika satu model tidak cukup kuat atau terlalu spesifik dalam memecahkan masalah tertentu.
	
  2. Homogeneous Ensemble Learning:
	
     Menggabungkan beberapa model yang berasal dari algoritma yang sama (misalnya, kumpulan pohon keputusan dalam Random Forest).
	
     Setiap model dalam ensambel dilatih dengan variasi data atau pengaturan parameter yang berbeda.
	
     Bertujuan untuk meningkatkan akurasi dengan menggunakan voting atau averaging dari banyak model serupa.
 

2.3 Pengenalan Bootstrap Aggregating | Bagging

![image](https://github.com/user-attachments/assets/ecdd2c0b-52ec-46fe-beb4-b1f2d1c03b27)


Pada gambar di atas merupakan contoh homogeneous ensemble learning, yang berarti kita menggunakan beberapa model yang sejenis untuk membentuk ensemble. Jika kita melakukan pelatihan (training) terhadap setiap model menggunakan training set yang sama, hal ini bisa menjadi sia-sia karena akan menghasilkan trained model yang identik. Semua model akan belajar pola yang sama dari data pelatihan, sehingga tidak ada variasi yang dapat meningkatkan performa ensemble.
Untuk mengatasi masalah ini, kita mengenal sebuah teknik yang disebut Bootstrap Aggregating atau lebih dikenal dengan istilah Bagging.
Bagging adalah metode ensemble learning di mana kita melakukan random sampling with replacement (pengambilan sampel acak dengan pengembalian) terhadap training set yang kita miliki untuk menghasilkan beberapa training set baru. Setiap training set baru yang dihasilkan dari proses ini disebut sebagai bag. Kemudian, model yang sejenis dilatih pada setiap bag tersebut.
 

2.4 Pengenalan Random Forest | Hutan Acak

![image](https://github.com/user-attachments/assets/b4f30825-d72e-4cdf-b7aa-1b4ccae4fa3e)

Random Forest adalah salah satu implementasi dari homogeneous ensemble learning yang menggunakan decision tree (pohon keputusan) sebagai model dasarnya. Dalam Random Forest, kita membuat sekumpulan pohon keputusan yang dilatih menggunakan teknik Bagging. Namun, selain menerapkan Bagging, Random Forest juga menerapkan konsep tambahan yang disebut features randomness atau randomisasi fitur.
Proses Prediksi di Random Forest:
Setelah sekumpulan pohon keputusan dilatih, Random Forest melakukan prediksi dengan menggabungkan hasil dari setiap pohon. Untuk masalah klasifikasi, hasil akhir ditentukan berdasarkan voting mayoritas dari semua pohon, sedangkan untuk masalah regresi, hasil akhir adalah rata-rata dari prediksi semua pohon.

2.5 Persiapan dataset | Iris Flower Dataset
 ![image](https://github.com/user-attachments/assets/a2cae6f1-a822-4e07-a92c-414d76283739)

2.6 Implementasi Random Forest Classifier dengan Scikit Learn
![image](https://github.com/user-attachments/assets/5d6bf6e3-d4c2-45da-9529-3bdbd7f29684)

2.7 Evaluasi model  dengan Classification Report
![image](https://github.com/user-attachments/assets/78a7c59c-6007-41c8-9e3f-5bab2995afb2)

